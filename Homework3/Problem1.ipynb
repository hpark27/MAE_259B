{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "loIShgzj2wO9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "# Print whether CPU or GPU is used.\n",
        "device = torch.device(\"cuda:0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Populate the Data for m, k1, k2, x, and y**"
      ],
      "metadata": {
        "id": "dzvcv4of-5pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.random.uniform(0,1,(1,1000)) \n",
        "k1 = np.random.uniform(0,1,(1,1000))\n",
        "k2 = k1\n",
        "posy =  9.81*m/(4*k1)\n",
        "posx =  0.5"
      ],
      "metadata": {
        "id": "hc5dZtWc29Pe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_test = np.random.uniform(0,1,(1,1000))\n",
        "k1_test = np.random.uniform(0,1,(1,1000))\n",
        "k2_test = k1_test\n",
        "posy_test =  9.81*m/(4*k1_test)\n",
        "posx_test = 0.5"
      ],
      "metadata": {
        "id": "OAlgf7ytAsTH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "lTERA7X3_Ig-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10 #Choose your own batch size"
      ],
      "metadata": {
        "id": "J4zhEBtC-qsN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_input = np.column_stack([np.transpose(k1),np.transpose(posy)])  # Fill in the input data format size : ()\n",
        "data_output = np.transpose(m) \n",
        "data_input_test = np.column_stack([np.transpose(k1_test),np.transpose(posy_test)])   # Fill in the input data format size : ()\n",
        "data_output_test =np.transpose(m_test) \n"
      ],
      "metadata": {
        "id": "TJLeSxrlAcFb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Data(data_input, data_output)\n",
        "train_set = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Data(data_input_test, data_output_test)\n",
        "test_set = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "QQ-K7AAL-1vd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "input_size =  2 # fill in the blank. \n",
        "hidden_layer_size = 10 # fill in the blank. \n",
        "learning_rate = 0.01 # fill in the blank. \n",
        "num_epochs = 3000 # fill in the blank. \n",
        "class RegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.dense_h1 = nn.Linear(in_features=input_size, out_features=hidden_size) \n",
        "        self.relu_h1 = nn.ReLU() # choose your own activation function\n",
        "        self.dense_h2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)  \n",
        "        self.relu_h2 = nn.ReLU() # choose your own activation function\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.dense_out = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        out = self.relu_h1(self.dense_h1(X))\n",
        "        out = self.relu_h2(self.dense_h2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.dense_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "m = RegressionModel(input_size=input_size, hidden_size=hidden_layer_size)\n",
        "\n",
        "cost_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate) \n",
        "\n",
        "all_losses = []\n",
        "for e in range(num_epochs):\n",
        "    batch_losses = []\n",
        "\n",
        "    for ix, (Xb, yb) in enumerate(train_set):\n",
        "\n",
        "        _X = Variable(Xb).float()\n",
        "        _y = Variable(yb).float()\n",
        "\n",
        "        #==========Forward pass===============\n",
        "\n",
        "        preds = m(_X)\n",
        "        loss = cost_func(preds, _y)\n",
        "\n",
        "        #==========backward pass==============\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.data)\n",
        "        all_losses.append(loss.data)\n",
        "\n",
        "    mbl = np.mean(np.sqrt(batch_losses)).round(3)\n",
        "\n",
        "    if e % 5 == 0:\n",
        "        print(\"Epoch [{}/{}], Batch loss: {}\".format(e, num_epochs, mbl))\n",
        "\n",
        "# prepares model for inference when trained with a dropout layer\n",
        "print(m.training)\n",
        "m.eval()\n",
        "print(m.training)\n",
        "\n",
        "test_batch_losses = []\n",
        "for _X, _y in test_set:\n",
        "\n",
        "    _X = Variable(_X).float()\n",
        "    _y = Variable(_y).float()\n",
        "\n",
        "    #apply model\n",
        "    test_preds = m(_X)\n",
        "    test_loss = cost_func(test_preds, _y)\n",
        "\n",
        "    test_batch_losses.append(test_loss.data)\n",
        "    print(\"Batch loss: {}\".format(test_loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7-y17kLgTO",
        "outputId": "728a49af-9ffd-426b-a9b2-282d77be19be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/3000], Batch loss: 0.42800000309944153\n",
            "Epoch [5/3000], Batch loss: 0.2409999966621399\n",
            "Epoch [10/3000], Batch loss: 0.23999999463558197\n",
            "Epoch [15/3000], Batch loss: 0.21400000154972076\n",
            "Epoch [20/3000], Batch loss: 0.2070000022649765\n",
            "Epoch [25/3000], Batch loss: 0.20900000631809235\n",
            "Epoch [30/3000], Batch loss: 0.20800000429153442\n",
            "Epoch [35/3000], Batch loss: 0.2150000035762787\n",
            "Epoch [40/3000], Batch loss: 0.20999999344348907\n",
            "Epoch [45/3000], Batch loss: 0.20900000631809235\n",
            "Epoch [50/3000], Batch loss: 0.2070000022649765\n",
            "Epoch [55/3000], Batch loss: 0.21199999749660492\n",
            "Epoch [60/3000], Batch loss: 0.21199999749660492\n",
            "Epoch [65/3000], Batch loss: 0.21199999749660492\n",
            "Epoch [70/3000], Batch loss: 0.2029999941587448\n",
            "Epoch [75/3000], Batch loss: 0.20900000631809235\n",
            "Epoch [80/3000], Batch loss: 0.21400000154972076\n",
            "Epoch [85/3000], Batch loss: 0.2709999978542328\n",
            "Epoch [90/3000], Batch loss: 0.289000004529953\n",
            "Epoch [95/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [100/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [105/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [110/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [115/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [120/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [125/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [130/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [135/3000], Batch loss: 0.289000004529953\n",
            "Epoch [140/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [145/3000], Batch loss: 0.289000004529953\n",
            "Epoch [150/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [155/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [160/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [165/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [170/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [175/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [180/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [185/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [190/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [195/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [200/3000], Batch loss: 0.289000004529953\n",
            "Epoch [205/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [210/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [215/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [220/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [225/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [230/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [235/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [240/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [245/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [250/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [255/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [260/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [265/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [270/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [275/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [280/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [285/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [290/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [295/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [300/3000], Batch loss: 0.289000004529953\n",
            "Epoch [305/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [310/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [315/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [320/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [325/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [330/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [335/3000], Batch loss: 0.289000004529953\n",
            "Epoch [340/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [345/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [350/3000], Batch loss: 0.289000004529953\n",
            "Epoch [355/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [360/3000], Batch loss: 0.289000004529953\n",
            "Epoch [365/3000], Batch loss: 0.289000004529953\n",
            "Epoch [370/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [375/3000], Batch loss: 0.289000004529953\n",
            "Epoch [380/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [385/3000], Batch loss: 0.28600001335144043\n",
            "Epoch [390/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [395/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [400/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [405/3000], Batch loss: 0.289000004529953\n",
            "Epoch [410/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [415/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [420/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [425/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [430/3000], Batch loss: 0.289000004529953\n",
            "Epoch [435/3000], Batch loss: 0.289000004529953\n",
            "Epoch [440/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [445/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [450/3000], Batch loss: 0.289000004529953\n",
            "Epoch [455/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [460/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [465/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [470/3000], Batch loss: 0.289000004529953\n",
            "Epoch [475/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [480/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [485/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [490/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [495/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [500/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [505/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [510/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [515/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [520/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [525/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [530/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [535/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [540/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [545/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [550/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [555/3000], Batch loss: 0.289000004529953\n",
            "Epoch [560/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [565/3000], Batch loss: 0.289000004529953\n",
            "Epoch [570/3000], Batch loss: 0.289000004529953\n",
            "Epoch [575/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [580/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [585/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [590/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [595/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [600/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [605/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [610/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [615/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [620/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [625/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [630/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [635/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [640/3000], Batch loss: 0.289000004529953\n",
            "Epoch [645/3000], Batch loss: 0.289000004529953\n",
            "Epoch [650/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [655/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [660/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [665/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [670/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [675/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [680/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [685/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [690/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [695/3000], Batch loss: 0.28600001335144043\n",
            "Epoch [700/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [705/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [710/3000], Batch loss: 0.289000004529953\n",
            "Epoch [715/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [720/3000], Batch loss: 0.289000004529953\n",
            "Epoch [725/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [730/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [735/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [740/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [745/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [750/3000], Batch loss: 0.289000004529953\n",
            "Epoch [755/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [760/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [765/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [770/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [775/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [780/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [785/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [790/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [795/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [800/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [805/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [810/3000], Batch loss: 0.289000004529953\n",
            "Epoch [815/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [820/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [825/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [830/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [835/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [840/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [845/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [850/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [855/3000], Batch loss: 0.289000004529953\n",
            "Epoch [860/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [865/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [870/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [875/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [880/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [885/3000], Batch loss: 0.289000004529953\n",
            "Epoch [890/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [895/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [900/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [905/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [910/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [915/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [920/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [925/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [930/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [935/3000], Batch loss: 0.289000004529953\n",
            "Epoch [940/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [945/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [950/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [955/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [960/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [965/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [970/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [975/3000], Batch loss: 0.289000004529953\n",
            "Epoch [980/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [985/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [990/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [995/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1000/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1005/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1010/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1015/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1020/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1025/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1030/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1035/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1040/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1045/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1050/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1055/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1060/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1065/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1070/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1075/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1080/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1085/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [1090/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1095/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1100/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1105/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1110/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1115/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1120/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1125/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1130/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1135/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1140/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1145/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1150/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1155/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1160/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1165/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1170/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1175/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1180/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1185/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1190/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1195/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1200/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1205/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1210/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1215/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1220/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1225/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1230/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1235/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1240/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1245/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1250/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1255/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1260/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1265/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1270/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1275/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1280/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1285/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1290/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1295/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1300/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1305/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1310/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1315/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1320/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1325/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1330/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1335/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1340/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [1345/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1350/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1355/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1360/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1365/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1370/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1375/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1380/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1385/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1390/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1395/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1400/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1405/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1410/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1415/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1420/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1425/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1430/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1435/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1440/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1445/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1450/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1455/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1460/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1465/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1470/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1475/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1480/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1485/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1490/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1495/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1500/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1505/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1510/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1515/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1520/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1525/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1530/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1535/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1540/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1545/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1550/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1555/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1560/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1565/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1570/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1575/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1580/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1585/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1590/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1595/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1600/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1605/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1610/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1615/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1620/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1625/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1630/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1635/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1640/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1645/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1650/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1655/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1660/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1665/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1670/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1675/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1680/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1685/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1690/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1695/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1700/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1705/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [1710/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1715/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1720/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1725/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1730/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1735/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1740/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1745/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1750/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1755/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1760/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1765/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1770/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1775/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1780/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1785/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1790/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1795/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1800/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1805/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1810/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1815/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1820/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1825/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1830/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1835/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1840/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1845/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1850/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1855/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1860/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1865/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1870/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1875/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1880/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1885/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1890/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1895/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1900/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1905/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1910/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1915/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1920/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1925/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1930/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1935/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1940/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1945/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1950/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1955/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1960/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1965/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1970/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [1975/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1980/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1985/3000], Batch loss: 0.289000004529953\n",
            "Epoch [1990/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [1995/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2000/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2005/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2010/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2015/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2020/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2025/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2030/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2035/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2040/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2045/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2050/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2055/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2060/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2065/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2070/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2075/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2080/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2085/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2090/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2095/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2100/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2105/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2110/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2115/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2120/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2125/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2130/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2135/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2140/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2145/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2150/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2155/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2160/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2165/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2170/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2175/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2180/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2185/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2190/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2195/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2200/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2205/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2210/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2215/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2220/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2225/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2230/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2235/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2240/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2245/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2250/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2255/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2260/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2265/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2270/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2275/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2280/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2285/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2290/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2295/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2300/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2305/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2310/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2315/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2320/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2325/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2330/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2335/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2340/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2345/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2350/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2355/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2360/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2365/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2370/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2375/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2380/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2385/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2390/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2395/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2400/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2405/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2410/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2415/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2420/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2425/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2430/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2435/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2440/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2445/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [2450/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2455/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2460/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2465/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2470/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2475/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2480/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2485/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2490/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2495/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2500/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2505/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2510/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2515/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2520/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2525/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2530/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2535/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2540/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2545/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2550/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2555/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2560/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2565/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2570/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2575/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2580/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2585/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2590/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2595/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2600/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2605/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2610/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2615/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2620/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2625/3000], Batch loss: 0.28999999165534973\n",
            "Epoch [2630/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2635/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2640/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2645/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2650/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2655/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2660/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2665/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2670/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2675/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2680/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2685/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2690/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2695/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2700/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2705/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2710/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2715/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2720/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2725/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2730/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2735/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2740/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2745/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2750/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2755/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2760/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2765/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2770/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2775/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2780/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2785/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2790/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2795/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2800/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2805/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2810/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2815/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2820/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2825/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2830/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2835/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2840/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2845/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2850/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2855/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2860/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2865/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2870/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2875/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2880/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2885/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2890/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2895/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2900/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2905/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2910/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2915/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2920/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2925/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2930/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2935/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2940/3000], Batch loss: 0.28700000047683716\n",
            "Epoch [2945/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2950/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2955/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2960/3000], Batch loss: 0.289000004529953\n",
            "Epoch [2965/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2970/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2975/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2980/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2985/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2990/3000], Batch loss: 0.2879999876022339\n",
            "Epoch [2995/3000], Batch loss: 0.2879999876022339\n",
            "True\n",
            "False\n",
            "Batch loss: 0.06478553265333176\n",
            "Batch loss: 0.06090155988931656\n",
            "Batch loss: 0.13011434674263\n",
            "Batch loss: 0.06632114201784134\n",
            "Batch loss: 0.08250720798969269\n",
            "Batch loss: 0.05199771374464035\n",
            "Batch loss: 0.11447495222091675\n",
            "Batch loss: 0.0718785747885704\n",
            "Batch loss: 0.07990758866071701\n",
            "Batch loss: 0.11285839974880219\n",
            "Batch loss: 0.12090368568897247\n",
            "Batch loss: 0.05724198743700981\n",
            "Batch loss: 0.058578915894031525\n",
            "Batch loss: 0.074832022190094\n",
            "Batch loss: 0.08771674335002899\n",
            "Batch loss: 0.07428999245166779\n",
            "Batch loss: 0.11181133985519409\n",
            "Batch loss: 0.02811383083462715\n",
            "Batch loss: 0.053804993629455566\n",
            "Batch loss: 0.09281116724014282\n",
            "Batch loss: 0.10433638095855713\n",
            "Batch loss: 0.08919195085763931\n",
            "Batch loss: 0.07673020660877228\n",
            "Batch loss: 0.08170414716005325\n",
            "Batch loss: 0.09235966950654984\n",
            "Batch loss: 0.05801444500684738\n",
            "Batch loss: 0.11308956146240234\n",
            "Batch loss: 0.12663567066192627\n",
            "Batch loss: 0.08601590245962143\n",
            "Batch loss: 0.07151678204536438\n",
            "Batch loss: 0.07924364507198334\n",
            "Batch loss: 0.10046570003032684\n",
            "Batch loss: 0.09056811034679413\n",
            "Batch loss: 0.08181685209274292\n",
            "Batch loss: 0.047850172966718674\n",
            "Batch loss: 0.06747278571128845\n",
            "Batch loss: 0.08487270027399063\n",
            "Batch loss: 0.03982770815491676\n",
            "Batch loss: 0.07137396931648254\n",
            "Batch loss: 0.11450417339801788\n",
            "Batch loss: 0.09692028164863586\n",
            "Batch loss: 0.06446437537670135\n",
            "Batch loss: 0.09925375878810883\n",
            "Batch loss: 0.08180172741413116\n",
            "Batch loss: 0.09596459567546844\n",
            "Batch loss: 0.07782473415136337\n",
            "Batch loss: 0.06243719533085823\n",
            "Batch loss: 0.12072332948446274\n",
            "Batch loss: 0.07848916947841644\n",
            "Batch loss: 0.08933515846729279\n",
            "Batch loss: 0.09044519811868668\n",
            "Batch loss: 0.09874807298183441\n",
            "Batch loss: 0.04268043115735054\n",
            "Batch loss: 0.06894570589065552\n",
            "Batch loss: 0.10478631407022476\n",
            "Batch loss: 0.06373948603868484\n",
            "Batch loss: 0.06380867213010788\n",
            "Batch loss: 0.07036855816841125\n",
            "Batch loss: 0.06085262820124626\n",
            "Batch loss: 0.06338105350732803\n",
            "Batch loss: 0.07772506773471832\n",
            "Batch loss: 0.033691875636577606\n",
            "Batch loss: 0.0657956525683403\n",
            "Batch loss: 0.10942475497722626\n",
            "Batch loss: 0.0634721964597702\n",
            "Batch loss: 0.09484963119029999\n",
            "Batch loss: 0.11104492843151093\n",
            "Batch loss: 0.07172112166881561\n",
            "Batch loss: 0.07689536362886429\n",
            "Batch loss: 0.06070477515459061\n",
            "Batch loss: 0.1020004153251648\n",
            "Batch loss: 0.06821416318416595\n",
            "Batch loss: 0.06429694592952728\n",
            "Batch loss: 0.046492818742990494\n",
            "Batch loss: 0.07805150747299194\n",
            "Batch loss: 0.10972710698843002\n",
            "Batch loss: 0.08774350583553314\n",
            "Batch loss: 0.0979098305106163\n",
            "Batch loss: 0.0957346111536026\n",
            "Batch loss: 0.10036931931972504\n",
            "Batch loss: 0.08681564033031464\n",
            "Batch loss: 0.07096538692712784\n",
            "Batch loss: 0.11735637485980988\n",
            "Batch loss: 0.043957095593214035\n",
            "Batch loss: 0.06543352454900742\n",
            "Batch loss: 0.0839935764670372\n",
            "Batch loss: 0.09259282052516937\n",
            "Batch loss: 0.051637910306453705\n",
            "Batch loss: 0.1017785519361496\n",
            "Batch loss: 0.08263617008924484\n",
            "Batch loss: 0.054052405059337616\n",
            "Batch loss: 0.10079674422740936\n",
            "Batch loss: 0.06659305840730667\n",
            "Batch loss: 0.04981653019785881\n",
            "Batch loss: 0.10069651901721954\n",
            "Batch loss: 0.08497349917888641\n",
            "Batch loss: 0.10427436977624893\n",
            "Batch loss: 0.1007923111319542\n",
            "Batch loss: 0.08076752722263336\n",
            "Batch loss: 0.10486187785863876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "plt.plot(np.array(all_losses))\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MDyYW52_8rOm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "a1578292-8b12-47bf-b24e-e881e0227ba4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAE9CAYAAADNvYHXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZcklEQVR4nO3dfYxld33f8ff33jsz++D1Ex4vWxuyJrggRwXsTqkpCDVQCA9V7DaEGKFkRS25oSQBpU2wG6kCKVUgaiBxoQEnQFYpCXadEFt5ADuLA60SGdZgGz9gvFhrYcv2jonN7np3Z+7Dt3+c365nl5nZWXvu3PnNvF/S1ZzzO+fc8z2/OXc+cx7uvZGZSJKkOrRGXYAkSVo6g1uSpIoY3JIkVcTgliSpIga3JEkVMbglSapIZ9QFLMU555yT27dvH3UZkiStiDvuuOPJzJycb1oVwb19+3Z279496jIkSVoREfHwQtM8VS5JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJF1l1wP7H/CN95fP+oy5Ak6Tmp4iNPl9Olv7WLTNj7kbePuhRJkk7Zujvizhx1BZIkPXfrLrglSaqZwS1JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVJGhBndEnBkRN0bEdyLi/oh4TUScHRG3RsSD5edZw6xBkqS1ZNhH3L8HfCkzXw68ErgfuBrYlZkXArvKuCRJWoKhBXdEnAG8HvgMQGbOZubTwGXAzjLbTuDyYdUgSdJaM8wj7guAaeBzEfGtiPjDiNgMbM3Mx8o8jwNbh1iDJElryjCDuwNcAvx+Zl4MPMMJp8UzM4Gcb+GIuCoidkfE7unp6SGWKUlSPYYZ3I8Aj2Tm7WX8RpogfyIitgGUn/vmWzgzr8vMqcycmpycHGKZkiTVY2jBnZmPA9+PiJeVpjcC9wE3AztK2w7gpmHVIEnSWtMZ8vP/MvD5iBgHHgLeQ/PPwg0RcSXwMPDOIdcgSdKaMdTgzsw7gal5Jr1xmOuVJGmt8pPTJEmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSaqIwS1JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVBGDW5KkinSG+eQRsRc4APSBXmZORcTZwPXAdmAv8M7MfGqYdUiStFasxBH3T2bmqzJzqoxfDezKzAuBXWVckiQtwShOlV8G7CzDO4HLR1CDJElVGnZwJ3BLRNwREVeVtq2Z+VgZfhzYOuQaJElaM4Z6jRt4XWY+GhHnArdGxHfmTszMjIicb8ES9FcBvPjFLx5ymZIk1WGoR9yZ+Wj5uQ/4IvBq4ImI2AZQfu5bYNnrMnMqM6cmJyeHWaYkSdUYWnBHxOaI2HJ0GHgzcA9wM7CjzLYDuGlYNUiStNYM81T5VuCLEXF0PX+SmV+KiG8AN0TElcDDwDuHWIMkSWvK0II7Mx8CXjlP+w+ANw5rvZIkrWV+cpokSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSaqIwS1JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklSRoQd3RLQj4lsR8Zdl/IKIuD0i9kTE9RExPuwaJElaK1biiPv9wP1zxj8KfDwzXwo8BVy5AjVIkrQmDDW4I+J84O3AH5bxAN4A3Fhm2QlcPswaJElaS4Z9xP27wK8DgzL+AuDpzOyV8UeA84ZcgyRJa8bQgjsi/i2wLzPveI7LXxURuyNi9/T09DJXJ0lSnYZ5xP1a4KcjYi/wBZpT5L8HnBkRnTLP+cCj8y2cmddl5lRmTk1OTg6xTEmS6jG04M7MazLz/MzcDlwBfCUz3w3cBryjzLYDuGlYNUiStNaM4n3cHwR+NSL20Fzz/swIapAkqUqdk8/y/GXm3wF/V4YfAl69EuuVJGmt8ZPTJEmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVWVJwR8TmiGiV4X8aET8dEWPDLU2SJJ1oqUfcXwM2RMR5wC3AzwN/NKyiJEnS/JYa3JGZh4B/D/yvzPxZ4CeGV5YkSZrPkoM7Il4DvBv4q9LWHk5JkiRpIUsN7g8A1wBfzMx7I+IlwG3DK0uSJM2ns5SZMvOrwFcByk1qT2bmrwyzMEmS9KOWelf5n0TE6RGxGbgHuC8ifm24pUmSpBMt9VT5RZm5H7gc+BvgApo7yyVJ0gpaanCPlfdtXw7cnJldIIdXliRJms9Sg/vTwF5gM/C1iPgxYP+wipIkSfNb6s1p1wLXzml6OCJ+cjglSZKkhSz15rQzIuJjEbG7PH6H5uhbkiStoKWeKv8scAB4Z3nsBz43rKIkSdL8lnSqHPjxzPyZOeMfjog7h1GQJEla2FKPuA9HxOuOjkTEa4HDiy0QERsi4usRcVdE3BsRHy7tF0TE7RGxJyKuj4jx516+JEnry1KD+xeBT0bE3ojYC3wC+I8nWWYGeENmvhJ4FfCWiLgU+Cjw8cx8KfAUcOVzqlySpHVoScGdmXeVAH4F8IrMvBh4w0mWycw8WEbHyiPLcjeW9p007w2XJElLsNQjbgAyc3/5BDWAXz3Z/BHRLtfC9wG3At8Dns7MXpnlEeC8U6lBkqT17JSC+wRxshkys5+ZrwLOB14NvHzJTx5x1dG3n01PTz+PMiVJWjueT3Av+SNPM/Npmq8BfQ1wZkQcvZv9fODRBZa5LjOnMnNqcnLyeZQpSdLasWhwR8SBiNg/z+MA8E9OsuxkRJxZhjcCbwLupwnwd5TZdgA3Pe+tkCRpnVj0fdyZueV5PPc2YGdEtGn+QbghM/8yIu4DvhARvwl8C/jM81iHJEnrylI/gOWUZebdwMXztD9Ec71bkiSdoudzjVuSJK0wg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSaqIwS1JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVGVpwR8SLIuK2iLgvIu6NiPeX9rMj4taIeLD8PGtYNUiStNYM84i7B/znzLwIuBR4X0RcBFwN7MrMC4FdZVySJC3B0II7Mx/LzG+W4QPA/cB5wGXAzjLbTuDyYdUgSdJasyLXuCNiO3AxcDuwNTMfK5MeB7auRA2SJK0FQw/uiDgN+DPgA5m5f+60zEwgF1juqojYHRG7p6enh12mJElVGGpwR8QYTWh/PjP/vDQ/ERHbyvRtwL75ls3M6zJzKjOnJicnh1mmJEnVGOZd5QF8Brg/Mz82Z9LNwI4yvAO4aVg1SJK01nSG+NyvBX4e+HZE3Fna/ivwEeCGiLgSeBh45xBrkCRpTRlacGfm/wNigclvHNZ6JUlay/zkNEmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSaqIwS1JUkUMbkmSKmJwS5JUEYNbkqSKGNySJFXE4JYkqSJDC+6I+GxE7IuIe+a0nR0Rt0bEg+XnWcNavyRJa9Ewj7j/CHjLCW1XA7sy80JgVxmXJElLNLTgzsyvAf94QvNlwM4yvBO4fFjrlyRpLVrpa9xbM/OxMvw4sHWF1y9JUtVGdnNaZiaQC02PiKsiYndE7J6enl7ByiRJWr1WOrifiIhtAOXnvoVmzMzrMnMqM6cmJydXrEBJklazlQ7um4EdZXgHcNMKr1+SpKoN8+1gfwr8A/CyiHgkIq4EPgK8KSIeBP5NGZckSUvUGdYTZ+a7Fpj0xmGtU5Kktc5PTpMkqSIGtyRJFTG4JUmqiMEtSVJFDG5JkipicEuSVBGDW5KkihjckiRVxOCWJKkiBrckSRUxuCVJqojBLUlSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSaqIwS1JUkUMbkmSKmJwLyIz+dlP/T1fvvfxUZciSRJgcC9qkPCNvU/xvs9/c9SlSJIEGNyL6g8SgFbEiCuRJKlhcC9ikCW47SVJ0iphJC3i6BF32yNuSdIqYXAvon/siNvgliStDgb3IgZe45YkrTIG9yKOnSr3iFuStEoY3Is4dqrcI25J0ioxkuCOiLdExAMRsScirh5FDVlCeTF7njgIwP7D3WGXI0nSkqx4cEdEG/gk8FbgIuBdEXHRStdx631PAPDrN97FJ77y4LxB/vG//S4As/3BitYmSdJCOiNY56uBPZn5EEBEfAG4DLhvJYu46o/vOG78f9zy3UXn3371Xw2zHElS5fZ+5O0rsp5RnCo/D/j+nPFHSttxIuKqiNgdEbunp6dXrDhJklazURxxL0lmXgdcBzA1NXXyC9JL9OB/fyu77t/H/Y/t53C3z9OHZgmC7mDATHfAxFiLIDhz0xgzvT7/YvvZ/OuXnUu3P2Ci0yKBme6A2f6AXn/AxrE2s/0Bm8Y7jHea/4NaAU/sn+HcLROMd1rs2XeQradvoFdOuZ+2oUMrgmdmehya7TO5ZYKDMz26/QG9fnL25nF6/SRa0GkFh2f7dFot+pmMd1r0+8lMr88gYazd3Dg3MdZu7oLP5hPfnpntccbGMWZ7AybG2nRaQW+QdHsDIqA3SAI4NNtnovPs/2/dQTLRabFpvE23l/zwcJeN422gqSWBiU6LCOj2k8yk2086raCfzbKHu302j3eY6Q2O1XdwpsfpG8YYZB7rs0OzfQBmegP6/Wzq7Q84fWOH6QMzdFotNoy1eOpQl03jbVoRTIy1mOkOONLts3G8TSaMt1skSUQw22v6OAI2j3fK9rfo9gdMH5jhBZsnmm1pl/7oDxhrtdg00ebgkR4RsHG8zWxvQKfVYrY3aPo8k5lu0+eDbLZ3rNOi10/areDgTI8zN44xU9Y/yGSs3SIzyYRNE23GWi0OzPToD5JWNO9WGAwgSQYJR7p9WhG0W812dlrBkW6z3z0z2+OsTeM8eXCGTWMdotXcNNmOZt/t9ZP+IBlvt+i0g4lOi8f3H6Fb9ido3t74g2dmeeEZGzg022O83SKimfeZmR6z/QFnbBzj0Gyfw7N9xtotTtvQ/Ik4NNMD4Eh3wLmnTxz73Y13Whya7TEYwBkbxzg402OQyZYNnWOvq6M3d2YmrWj2k36/2ZcPzvQ40u1z7pYNDDI5cKTHC04b54eHuwwy6fWTzRMdZrp9Nk10OHikx8axdtlXB3T7AzZPdBhvt9h/pMtg0NR0pOyDAAdmumyZGIMAEp6Z7THRaXGk12zv/sNdJjoteoOmvg1jLbr95NBsj4lOmwjYNN7mmZk+WzZ06PYHDI7+RcrmUlpm0hskg0xO3zhGZtPfG8bazPT6HL0S12o1v99ef0Amx5bZNN4my2vsUOn7jWNtuv3BsfYnD86QCadNdBjrtDg82ydJOq0WvX6znw6yeX0enOkRAAFBs852K+gPkrF2s79OdNpkJv1s6vzHg7Ocfdo4B440/dOKps5WK9hQ/r4MBsm+AzO88IwNtKLp68OzfZ4+1OXMTWNA826co6+Nbn/AxvE2R8r+EhF0+wOeOjTLC0/fwMbxdnk9BIe7ZR6afTtpPvxqkMlMb8CGsRZj7eZ5n5ntMdMbsGVDh7FWiyO9ps8GZVu6vUHZD5t9EjjWR/1BcnCmx8bxNp1Wiyj70lir2eanDjWvkYMzvebvb7dP0LxONk20OdIdMNPrH9uvJzotDs32OXfLxFJj6HmLpdyktawrjHgN8KHM/Kkyfg1AZv7WQstMTU3l7t27V6hCSZJGKyLuyMyp+aaN4lT5N4ALI+KCiBgHrgBuHkEdkiRVZ8VPlWdmLyJ+Cfgy0AY+m5n3rnQdkiTVaCTXuDPzr4G/HsW6JUmqmZ+cJklSRQxuSZIqYnBLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVWfGPPH0uImIaeHgZn/Ic4MllfL7a2R/Psi+OZ38cz/54ln1xvOXujx/LzMn5JlQR3MstInYv9Bmw65H98Sz74nj2x/Hsj2fZF8dbyf7wVLkkSRUxuCVJqsh6De7rRl3AKmN/PMu+OJ79cTz741n2xfFWrD/W5TVuSZJqtV6PuCVJqtK6C+6IeEtEPBAReyLi6lHXs5wiYm9EfDsi7oyI3aXt7Ii4NSIeLD/PKu0REdeWfrg7Ii6Z8zw7yvwPRsSOOe3/vDz/nrJsrPxWLiwiPhsR+yLinjltQ9/+hdYxSgv0xYci4tGyf9wZEW+bM+2asl0PRMRPzWmf9/USERdExO2l/fqIGC/tE2V8T5m+fWW2eHER8aKIuC0i7ouIeyPi/aV93e0fi/TFutw/ImJDRHw9Iu4q/fHh0n7K27Bc/XRSmbluHkAb+B7wEmAcuAu4aNR1LeP27QXOOaHtt4Gry/DVwEfL8NuAvwECuBS4vbSfDTxUfp5Vhs8q075e5o2y7FtHvc0nbOvrgUuAe1Zy+xdaxyrsiw8B/2WeeS8qr4UJ4ILyGmkv9noBbgCuKMOfAt5bhv8T8KkyfAVw/aj7otSyDbikDG8Bvlu2e93tH4v0xbrcP8rv67QyPAbcXn6Pp7QNy9lPJ6151J22wr+g1wBfnjN+DXDNqOtaxu3by48G9wPAtjK8DXigDH8aeNeJ8wHvAj49p/3TpW0b8J057cfNt1oewHaOD6uhb/9C6xj1Y56++BDz/2E+7nUAfLm8VuZ9vZQ/dE8CndJ+bL6jy5bhTpkvRt0X82zzTcCb1vP+MU9frPv9A9gEfBP4l6e6DcvZTyd7rLdT5ecB358z/khpWysSuCUi7oiIq0rb1sx8rAw/Dmwtwwv1xWLtj8zTvtqtxPYvtI7V6JfKqd/Pzjlle6p98QLg6czsndB+3HOV6T8s868a5dTmxTRHVut6/zihL2Cd7h8R0Y6IO4F9wK00R8inug3L2U+LWm/Bvda9LjMvAd4KvC8iXj93Yjb/1q3btxGsxPav8j7+feDHgVcBjwG/M9pyVl5EnAb8GfCBzNw/d9p62z/m6Yt1u39kZj8zXwWcD7waePmIS1rUegvuR4EXzRk/v7StCZn5aPm5D/gizQ74RERsAyg/95XZF+qLxdrPn6d9tVuJ7V9oHatKZj5R/kANgD+g2T/g1PviB8CZEdE5of245yrTzyjzj1xEjNEE1ecz889L87rcP+bri/W+fwBk5tPAbTSnrU91G5aznxa13oL7G8CF5U6+cZobC24ecU3LIiI2R8SWo8PAm4F7aLbv6J2vO2iuZ1Haf6HcPXsp8MNyOu/LwJsj4qxyquzNNNddHgP2R8Sl5W7ZX5jzXKvZSmz/QutYVY6GR/HvaPYPaOq/otwtewFwIc2NVvO+XspR423AO8ryJ/br0b54B/CVMv9Ild/ZZ4D7M/Njcyatu/1job5Yr/tHRExGxJlleCPN9f77OfVtWM5+WtyobwZY6QfN3aLfpbmG8RujrmcZt+slNHcr3gXce3TbaK6j7AIeBP4WOLu0B/DJ0g/fBqbmPNd/APaUx3vmtE/RvJi/B3yCVXRTSanvT2lO8XVprhdduRLbv9A6VmFf/HHZ1rvLH5ltc+b/jbJdDzDn3QILvV7K/vb10kf/B5go7RvK+J4y/SWj7otS1+toTlHfDdxZHm9bj/vHIn2xLvcP4BXAt8p23wP8t+e6DcvVTyd7+MlpkiRVZL2dKpckqWoGtyRJFTG4JUmqiMEtSVJFDG5JkipicEtrVET049lverozlvHb8CJie8z55jFJK6dz8lkkVepwNh/jKGkN8YhbWmei+d72347mu6O/HhEvLe3bI+Ir5UsmdkXEi0v71oj4YjTfV3xXRPyr8lTtiPiDaL7D+JbyqVNExK9E813Pd0fEF0a0mdKaZXBLa9fGE06V/9ycaT/MzH9G8wlfv1va/iewMzNfAXweuLa0Xwt8NTNfSfMd3/eW9guBT2bmTwBPAz9T2q8GLi7P84vD2jhpvfKT06Q1KiIOZuZp87TvBd6QmQ+VL5t4PDNfEBFP0nzMZbe0P5aZ50TENHB+Zs7MeY7twK2ZeWEZ/yAwlpm/GRFfAg4CfwH8RWYeHPKmSuuKR9zS+pQLDJ+KmTnDfZ69Z+btNJ/zfQnwjTnffiRpGRjc0vr0c3N+/kMZ/nuaby4CeDfwf8vwLuC9ABHRjogzFnrSiGgBL8rM24AP0nzl4Y8c9Ut67vxPWFq7NkbEnXPGv5SZR98SdlZE3E1z1Pyu0vbLwOci4teAaeA9pf39wHURcSXNkfV7ab55bD5t4H+XcA/g2my+41jSMvEat7TOlGvcU5n55KhrkXTqPFUuSVJFPOKWJKkiHnFLklQRg1uSpIoY3JIkVcTgliSpIga3JEkVMbglSarI/wc082NEVHSGiAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "# result = model(X)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "3ihpvYBqJ0EA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVmQVs-lKODQ"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}