{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "loIShgzj2wO9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "# Print whether CPU or GPU is used.\n",
        "device = torch.device(\"cuda:0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Populate the Data for m, k1, k2, x, and y**"
      ],
      "metadata": {
        "id": "dzvcv4of-5pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.random.uniform(0,1,(1,1000)) \n",
        "k1 = np.random.uniform(0,1,(1,1000))\n",
        "k2 = k1\n",
        "posy =  9.81*m/(4*k1)\n",
        "posx =  0.5"
      ],
      "metadata": {
        "id": "hc5dZtWc29Pe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_test = np.random.uniform(0,1,(1,1000))\n",
        "k1_test = np.random.uniform(0,1,(1,1000))\n",
        "k2_test = k1_test\n",
        "posy_test =  9.81*m/(4*k1_test)\n",
        "posx_test = 0.5"
      ],
      "metadata": {
        "id": "OAlgf7ytAsTH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "lTERA7X3_Ig-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10 #Choose your own batch size"
      ],
      "metadata": {
        "id": "J4zhEBtC-qsN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_input = np.column_stack([np.transpose(k1),np.transpose(posy)])  # Fill in the input data format size : ()\n",
        "data_output = np.transpose(m) \n",
        "data_input_test = np.column_stack([np.transpose(k1_test),np.transpose(posy_test)])   # Fill in the input data format size : ()\n",
        "data_output_test =np.transpose(m_test) \n"
      ],
      "metadata": {
        "id": "TJLeSxrlAcFb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Data(data_input, data_output)\n",
        "train_set = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Data(data_input_test, data_output_test)\n",
        "test_set = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "QQ-K7AAL-1vd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "input_size =  2 # fill in the blank. \n",
        "hidden_layer_size = 10 # fill in the blank. \n",
        "learning_rate = 0.01 # fill in the blank. \n",
        "num_epochs = 3000 # fill in the blank. \n",
        "class RegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.dense_h1 = nn.Linear(in_features=input_size, out_features=hidden_size) \n",
        "        self.relu_h1 = nn.ReLU() # choose your own activation function\n",
        "        self.dense_h2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)  \n",
        "        self.relu_h2 = nn.ReLU() # choose your own activation function\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.dense_out = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        out = self.relu_h1(self.dense_h1(X))\n",
        "        out = self.relu_h2(self.dense_h2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.dense_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "m = RegressionModel(input_size=input_size, hidden_size=hidden_layer_size)\n",
        "\n",
        "cost_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate) \n",
        "\n",
        "all_losses = []\n",
        "for e in range(num_epochs):\n",
        "    batch_losses = []\n",
        "\n",
        "    for ix, (Xb, yb) in enumerate(train_set):\n",
        "\n",
        "        _X = Variable(Xb).float()\n",
        "        _y = Variable(yb).float()\n",
        "\n",
        "        #==========Forward pass===============\n",
        "\n",
        "        preds = m(_X)\n",
        "        loss = cost_func(preds, _y)\n",
        "\n",
        "        #==========backward pass==============\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.data)\n",
        "        all_losses.append(loss.data)\n",
        "\n",
        "    mbl = np.mean(np.sqrt(batch_losses)).round(3)\n",
        "\n",
        "    if e % 5 == 0:\n",
        "        print(\"Epoch [{}/{}], Batch loss: {}\".format(e, num_epochs, mbl))\n",
        "\n",
        "# prepares model for inference when trained with a dropout layer\n",
        "print(m.training)\n",
        "m.eval()\n",
        "print(m.training)\n",
        "\n",
        "test_batch_losses = []\n",
        "for _X, _y in test_set:\n",
        "\n",
        "    _X = Variable(_X).float()\n",
        "    _y = Variable(_y).float()\n",
        "\n",
        "    #apply model\n",
        "    test_preds = m(_X)\n",
        "    test_loss = cost_func(test_preds, _y)\n",
        "\n",
        "    test_batch_losses.append(test_loss.data)\n",
        "    # print(\"Batch loss: {}\".format(test_loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7-y17kLgTO",
        "outputId": "325c286b-69ea-4760-f055-2e14992e0d29"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/3000], Batch loss: 0.22699999809265137\n",
            "Epoch [5/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [10/3000], Batch loss: 0.164000004529953\n",
            "Epoch [15/3000], Batch loss: 0.16500000655651093\n",
            "Epoch [20/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [25/3000], Batch loss: 0.164000004529953\n",
            "Epoch [30/3000], Batch loss: 0.16599999368190765\n",
            "Epoch [35/3000], Batch loss: 0.16300000250339508\n",
            "Epoch [40/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [45/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [50/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [55/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [60/3000], Batch loss: 0.164000004529953\n",
            "Epoch [65/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [70/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [75/3000], Batch loss: 0.16200000047683716\n",
            "Epoch [80/3000], Batch loss: 0.16200000047683716\n",
            "Epoch [85/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [90/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [95/3000], Batch loss: 0.17000000178813934\n",
            "Epoch [100/3000], Batch loss: 0.16500000655651093\n",
            "Epoch [105/3000], Batch loss: 0.16599999368190765\n",
            "Epoch [110/3000], Batch loss: 0.17000000178813934\n",
            "Epoch [115/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [120/3000], Batch loss: 0.17100000381469727\n",
            "Epoch [125/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [130/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [135/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [140/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [145/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [150/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [155/3000], Batch loss: 0.13500000536441803\n",
            "Epoch [160/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [165/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [170/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [175/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [180/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [185/3000], Batch loss: 0.1379999965429306\n",
            "Epoch [190/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [195/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [200/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [205/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [210/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [215/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [220/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [225/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [230/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [235/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [240/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [245/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [250/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [255/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [260/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [265/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [270/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [275/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [280/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [285/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [290/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [295/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [300/3000], Batch loss: 0.13899999856948853\n",
            "Epoch [305/3000], Batch loss: 0.13899999856948853\n",
            "Epoch [310/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [315/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [320/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [325/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [330/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [335/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [340/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [345/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [350/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [355/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [360/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [365/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [370/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [375/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [380/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [385/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [390/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [395/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [400/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [405/3000], Batch loss: 0.16200000047683716\n",
            "Epoch [410/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [415/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [420/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [425/3000], Batch loss: 0.14000000059604645\n",
            "Epoch [430/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [435/3000], Batch loss: 0.14000000059604645\n",
            "Epoch [440/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [445/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [450/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [455/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [460/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [465/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [470/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [475/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [480/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [485/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [490/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [495/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [500/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [505/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [510/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [515/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [520/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [525/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [530/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [535/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [540/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [545/3000], Batch loss: 0.13500000536441803\n",
            "Epoch [550/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [555/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [560/3000], Batch loss: 0.13699999451637268\n",
            "Epoch [565/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [570/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [575/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [580/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [585/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [590/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [595/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [600/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [605/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [610/3000], Batch loss: 0.1379999965429306\n",
            "Epoch [615/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [620/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [625/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [630/3000], Batch loss: 0.13699999451637268\n",
            "Epoch [635/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [640/3000], Batch loss: 0.164000004529953\n",
            "Epoch [645/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [650/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [655/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [660/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [665/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [670/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [675/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [680/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [685/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [690/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [695/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [700/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [705/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [710/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [715/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [720/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [725/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [730/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [735/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [740/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [745/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [750/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [755/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [760/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [765/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [770/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [775/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [780/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [785/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [790/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [795/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [800/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [805/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [810/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [815/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [820/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [825/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [830/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [835/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [840/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [845/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [850/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [855/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [860/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [865/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [870/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [875/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [880/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [885/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [890/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [895/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [900/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [905/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [910/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [915/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [920/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [925/3000], Batch loss: 0.14000000059604645\n",
            "Epoch [930/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [935/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [940/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [945/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [950/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [955/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [960/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [965/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [970/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [975/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [980/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [985/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [990/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [995/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1000/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1005/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1010/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1015/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1020/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1025/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1030/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1035/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1040/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [1045/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1050/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1055/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [1060/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1065/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1070/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [1075/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1080/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1085/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1090/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1095/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1100/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1105/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1110/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1115/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1120/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1125/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1130/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1135/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1140/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [1145/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1150/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1155/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1160/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1165/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1170/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1175/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1180/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1185/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [1190/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1195/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1200/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1205/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1210/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [1215/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [1220/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [1225/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1230/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1235/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1240/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1245/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1250/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1255/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1260/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1265/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1270/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [1275/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [1280/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1285/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1290/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [1295/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1300/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1305/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [1310/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [1315/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1320/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1325/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1330/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1335/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1340/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1345/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1350/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1355/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1360/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1365/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1370/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1375/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1380/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1385/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1390/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1395/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1400/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1405/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1410/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1415/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1420/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1425/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1430/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1435/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [1440/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1445/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1450/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1455/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1460/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1465/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1470/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [1475/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1480/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1485/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1490/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1495/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1500/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1505/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1510/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1515/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1520/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1525/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1530/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1535/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1540/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1545/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1550/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1555/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1560/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1565/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1570/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1575/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1580/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1585/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1590/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1595/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1600/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1605/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1610/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1615/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1620/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1625/3000], Batch loss: 0.16200000047683716\n",
            "Epoch [1630/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1635/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1640/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1645/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1650/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1655/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1660/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [1665/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1670/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1675/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1680/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1685/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1690/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1695/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1700/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1705/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1710/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1715/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1720/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1725/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1730/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [1735/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1740/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1745/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1750/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1755/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1760/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1765/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1770/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1775/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1780/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1785/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1790/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1795/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1800/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1805/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [1810/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1815/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1820/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1825/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1830/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1835/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1840/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1845/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1850/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1855/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1860/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [1865/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1870/3000], Batch loss: 0.17000000178813934\n",
            "Epoch [1875/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1880/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1885/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1890/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1895/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1900/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1905/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1910/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1915/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [1920/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1925/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1930/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1935/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [1940/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [1945/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [1950/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [1955/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [1960/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1965/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [1970/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1975/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [1980/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [1985/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [1990/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [1995/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2000/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2005/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2010/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2015/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2020/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2025/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2030/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2035/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2040/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2045/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2050/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2055/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2060/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2065/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2070/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2075/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2080/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2085/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2090/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2095/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2100/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2105/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [2110/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2115/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2120/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2125/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [2130/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2135/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2140/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2145/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2150/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2155/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2160/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2165/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2170/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2175/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2180/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2185/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2190/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2195/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2200/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2205/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2210/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2215/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2220/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2225/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2230/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2235/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2240/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2245/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2250/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2255/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2260/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2265/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2270/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2275/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [2280/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2285/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2290/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2295/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2300/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2305/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2310/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2315/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2320/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2325/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [2330/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2335/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2340/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2345/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2350/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2355/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2360/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2365/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2370/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2375/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2380/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2385/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2390/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [2395/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [2400/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2405/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2410/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2415/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2420/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2425/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2430/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2435/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2440/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2445/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2450/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2455/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2460/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2465/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2470/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2475/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2480/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2485/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2490/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2495/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2500/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2505/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2510/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2515/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2520/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2525/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2530/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2535/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2540/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2545/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2550/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2555/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2560/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2565/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2570/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2575/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2580/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2585/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2590/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [2595/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2600/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2605/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2610/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2615/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2620/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2625/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2630/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2635/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2640/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2645/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2650/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2655/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2660/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2665/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2670/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2675/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2680/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2685/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2690/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2695/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [2700/3000], Batch loss: 0.14100000262260437\n",
            "Epoch [2705/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2710/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2715/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [2720/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2725/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2730/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2735/3000], Batch loss: 0.15800000727176666\n",
            "Epoch [2740/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2745/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2750/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2755/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2760/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2765/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2770/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2775/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2780/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2785/3000], Batch loss: 0.14900000393390656\n",
            "Epoch [2790/3000], Batch loss: 0.15299999713897705\n",
            "Epoch [2795/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2800/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2805/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2810/3000], Batch loss: 0.1589999943971634\n",
            "Epoch [2815/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2820/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2825/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2830/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2835/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2840/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2845/3000], Batch loss: 0.14800000190734863\n",
            "Epoch [2850/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2855/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2860/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2865/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2870/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2875/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2880/3000], Batch loss: 0.15000000596046448\n",
            "Epoch [2885/3000], Batch loss: 0.1420000046491623\n",
            "Epoch [2890/3000], Batch loss: 0.14499999582767487\n",
            "Epoch [2895/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2900/3000], Batch loss: 0.15199999511241913\n",
            "Epoch [2905/3000], Batch loss: 0.1599999964237213\n",
            "Epoch [2910/3000], Batch loss: 0.1550000011920929\n",
            "Epoch [2915/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2920/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2925/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2930/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2935/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2940/3000], Batch loss: 0.14300000667572021\n",
            "Epoch [2945/3000], Batch loss: 0.15700000524520874\n",
            "Epoch [2950/3000], Batch loss: 0.1469999998807907\n",
            "Epoch [2955/3000], Batch loss: 0.1509999930858612\n",
            "Epoch [2960/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2965/3000], Batch loss: 0.16099999845027924\n",
            "Epoch [2970/3000], Batch loss: 0.15600000321865082\n",
            "Epoch [2975/3000], Batch loss: 0.14399999380111694\n",
            "Epoch [2980/3000], Batch loss: 0.15399999916553497\n",
            "Epoch [2985/3000], Batch loss: 0.1459999978542328\n",
            "Epoch [2990/3000], Batch loss: 0.16500000655651093\n",
            "Epoch [2995/3000], Batch loss: 0.14900000393390656\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "plt.plot(np.array(all_losses))\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MDyYW52_8rOm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "b3480a65-8870-45a3-8511-a78a55c0b25a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "# result = model(X)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "3ihpvYBqJ0EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVmQVs-lKODQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}